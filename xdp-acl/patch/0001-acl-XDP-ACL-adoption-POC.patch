From 3c25f54d4f031b81e5d664b37e2687c5852e3ded Mon Sep 17 00:00:00 2001
From: Oleg Ananiev <oleg.k.ananiev@gmail.com>
Date: Mon, 7 Jul 2025 18:40:51 +0100
Subject: [PATCH] acl:  XDP-ACL adoption (POC)
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit

This a Proof of Concept to adopt DPDK ACL packet classificatio method
within XDP program.
The patch consists of three main parts:
1) Changes in ACL library to open BPF MAPs and load generated ACL
   context (transisions and rules arrays) into the provide BPF MAPs.
2) User-space program (dpdk-acl-bpf) that performs build of provided filter
   rules into ACL context and then uploads generated context into BPF MAPs.
   Note that dpdk-acl-bpf program is a reworked version of app/test-acl
   program provided by DPDK.
3) XDP program that performs actual search over input packetâ€™s data using
   filled by 2) BPF MAPs.

how to try
+++++++++++

1) Get DPDK LTS (24.11) version and apply the patch:

$ git clone -v  https://dpdk.org/git/dpdk dpdk-acl-bpf
$ cd dpdk-acl-bpf
$ git checkout v24.11
$ git am -3 <this-patch-name>

2) Build DPDK (it could take some time):

$ meson setup --prefix=${PWD}/x86_64-default-linuxapp-gcc-dbg-install --werror -Dbuildtype=debug -Dmachine=default x86_64-default-linuxapp-gcc-dbg
$ ninja -v -j 8 -C x86_64-default-linuxapp-gcc-dbg/

3) Compile XDP program. Note that clang version 19 or higher is
   required. Previous clang versions might crash when trying to compile
   it:

$ clang -O2 -g -Wall -target bpf -I./lib/acl/ -I./ -c app/acl-bpf/xdp_acl4.c -o app/acl-bpf/xdp_acl4.o

4) Load compiled XDP program into the kernel, for NIC you plan to use it
   with. Then verify that it was loaded and get created BPF MAP IDs.

$ sudo xdp-loader load -vv <your_NIC_device_name> ./app/acl-bpf/xdp_acl4.o

$ bpftool map show
  ....
  1345: array  name acl_ctx  flags 0x0
        key 4B  value 1360B  max_entries 1  memlock 1680B
        btf_id 2083
  1346: array  name acl_trans  flags 0x0
        key 4B  value 8B  max_entries 4194304  memlock 33554752B
        btf_id 2083
  1347: array  name acl_rule  flags 0x0
        key 4B  value 48B  max_entries 65536  memlock 3146048B
        btf_id 2083

5) Generate ACL context for filtering rules and uploaded generated
   context into XDP BPF MAPs. Note that user required to provide BPF MAP
   IDs in a specific order:
   --bpf="ID(acl_ctx):ID(acl_trans):ID(acl_rule)", in that particular
   case: --bpf="1345:1346:1347".
   Parameters before "--" are DPDK specific, and better be unmodified.
   For more information about them please refer to DPDK users guide.
   --rulesf - specifies file witht the input ruleset (in classbench-ng
     format.
   --rulenum - specifies max number of rules to load. I used to provide
     some big value, to make sure that all rules will be uploaded.

$ sudo ./x86_64-default-linuxapp-gcc-dbg/app/dpdk-acl-bpf -n 1 --lcores='0' --no-pci --no-huge -- --rulesf=<your_input_rule_file> --rulenum=100000 --bpf="1345:1346:1347"

Also 'dpdk-acl-bpf' for testing purposes allows user to perform search
using uploaded BPF MAPs in user-space against provided classbench-ng
trace file. As an example:

$ sudo ./x86_64-default-linuxapp-gcc-dbg/app/dpdk-acl-bpf -n 1 --lcores='0' --no-pci --no-huge -- --rulesf=./test-acl-input/acl1v4_10k_rule --tracef=./test-acl-input/acl1v4_10k_trace --iter=1 --verbose=3 --rulenum=100000 --tracenum=100000 --bpf="1345:1346:1347"

Will perform search for each input trace and print matching result.
As an example, output line:
ipv4_5tuple: 97850, category: 0, result: 5183
means that for trace with ID '97850' matching rule ID is 5183.
Input trace shall also contain expected mathcing rule and these IDs
should be identical:
$cat -n ./dts/dep/test-acl-input/acl1v4_10k_trace | grep 97850
 97850  3869543598      601217244       65535   61009   6       0       5183

6) At that point BPF MAP contents should be loaded, and XDP ACL program
   should be ready for action. For the NIC it installed on, it should
   intercept and drop all packets that match any of the provided rule,
   while the rest of the traffic should remain intact. Note that each
   acl_rule has an atomic counter for matched (dropped) packets.
   User can examine its value with 'bpftool map lookup' command.
   Let say to examine rule with ID 5183 (note that lookup accepts key as
   an array of bytes, starting from less significant one):

$ sudo bpftool map lookup name acl_rule key 63 20 00 00
{
    "key": 5183,
    "value": {
        "id": 5183,
        "action": "XDP_DROP",
        "rule": {
            "proto": 6,
            "proto_mask": 255,
            "ip_src": 3869543584,
            "ip_src_mask_len": 32,
            "ip_dst": 601217244,
            "ip_dst_mask_len": 32,
            "port_src_low": 0,
            "port_src_high": 65535,
            "port_dst_low": 61000,
            "port_dst_high": 61009
        },
        "num_packet": 13
    }
}

Signed-off-by: Oleg Ananiev <oleg.k.ananiev@gmail.com>
---
 app/acl-bpf/acl_internal.h |   49 ++
 app/acl-bpf/acl_xdp.h      |   67 ++
 app/acl-bpf/main.c         | 1311 ++++++++++++++++++++++++++++++++++++
 app/acl-bpf/meson.build    |   11 +
 app/acl-bpf/xdp_acl4.c     |  275 ++++++++
 app/meson.build            |    1 +
 lib/acl/acl_bpf.c          |  345 ++++++++++
 lib/acl/acl_bpf.h          |   36 +
 lib/acl/meson.build        |   15 +
 lib/acl/rte_acl.h          |   44 ++
 10 files changed, 2154 insertions(+)
 create mode 100644 app/acl-bpf/acl_internal.h
 create mode 100644 app/acl-bpf/acl_xdp.h
 create mode 100644 app/acl-bpf/main.c
 create mode 100644 app/acl-bpf/meson.build
 create mode 100644 app/acl-bpf/xdp_acl4.c
 create mode 100644 lib/acl/acl_bpf.c
 create mode 100644 lib/acl/acl_bpf.h

diff --git a/app/acl-bpf/acl_internal.h b/app/acl-bpf/acl_internal.h
new file mode 100644
index 0000000000..9367009ddb
--- /dev/null
+++ b/app/acl-bpf/acl_internal.h
@@ -0,0 +1,49 @@
+
+#ifndef _ACL_INTERNAL_H_
+#define _ACL_INTERNAL_H_
+
+/*
+ * !!! copy of internal defines from ACL library
+ * we need it here to implement exactly the same classify() method
+ * within our XDP program.
+ */
+
+/** Mask value of type "tp" for the first "ln" bit set. */
+#define RTE_LEN2MASK(ln, tp)    \
+	((tp)((uint64_t)-1 >> (sizeof(uint64_t) * CHAR_BIT - (ln))))
+
+enum {
+        RTE_ACL_TYPE_SHIFT = 29,
+        RTE_ACL_MAX_INDEX = RTE_LEN2MASK(RTE_ACL_TYPE_SHIFT, uint32_t),
+        RTE_ACL_MAX_PRIORITY = RTE_ACL_MAX_INDEX,
+        RTE_ACL_MIN_PRIORITY = 1,
+};
+
+#define RTE_ACL_NODE_DFA	(0 << RTE_ACL_TYPE_SHIFT)
+#define RTE_ACL_NODE_SINGLE	(1U << RTE_ACL_TYPE_SHIFT)
+#define RTE_ACL_NODE_QRANGE	(3U << RTE_ACL_TYPE_SHIFT)
+#define RTE_ACL_NODE_MATCH	(4U << RTE_ACL_TYPE_SHIFT)
+#define RTE_ACL_NODE_TYPE	(7U << RTE_ACL_TYPE_SHIFT)
+#define RTE_ACL_NODE_UNDEFINED	UINT32_MAX
+
+#define RTE_ACL_QUAD_MAX	5
+#define RTE_ACL_QUAD_SIZE	4
+#define RTE_ACL_QUAD_SINGLE	UINT64_C(0x7f7f7f7f00000000)
+
+#define RTE_ACL_SINGLE_TRIE_SIZE	2000
+
+#define RTE_ACL_DFA_MAX		UINT8_MAX
+#define RTE_ACL_DFA_SIZE	(UINT8_MAX + 1)
+
+#define RTE_ACL_DFA_GR64_SIZE	64
+#define RTE_ACL_DFA_GR64_NUM	(RTE_ACL_DFA_SIZE / RTE_ACL_DFA_GR64_SIZE)
+#define RTE_ACL_DFA_GR64_BIT	\
+	(CHAR_BIT * sizeof(uint32_t) / RTE_ACL_DFA_GR64_NUM)
+
+#define RTE_ACL_NODE_INDEX	((uint32_t)~RTE_ACL_NODE_TYPE)
+
+#define SCALAR_QRANGE_MULT	0x01010101
+#define SCALAR_QRANGE_MASK	0x7f7f7f7f
+#define SCALAR_QRANGE_MIN	0x80808080
+
+#endif /* _ACL_INTERNAL_H_ */
diff --git a/app/acl-bpf/acl_xdp.h b/app/acl-bpf/acl_xdp.h
new file mode 100644
index 0000000000..1fc0f890f2
--- /dev/null
+++ b/app/acl-bpf/acl_xdp.h
@@ -0,0 +1,67 @@
+
+#ifndef _ACL_XDP_H_
+#define _ACL_XDP_H_
+
+/**
+ * Define common structures that are used both by user-space and XDP programs.
+ */
+
+/**
+ * IPv4 header fields we are searching through:
+ * - IP protocol value (8 bits)
+ * - IP source address (32 bits)
+ * - IP destination address (32 bits)
+ * - L4 protocol (UDP/TCP) source port number (16 bits)
+ * - L4 protocol (UDP/TCP) destination port number (16 bits)
+ */
+struct ipv4_5tuple {
+        uint8_t  proto;
+        uint32_t ip_src;
+        uint32_t ip_dst;
+        uint16_t port_src;
+        uint16_t port_dst;
+};
+
+struct ipv4_5tuple_rule {
+	uint8_t proto;                 /**< IPv4 protocol ID. */
+	uint8_t proto_mask;            /**< IPv4 protocol ID mask. */
+	uint32_t ip_src;               /**< IPv4 source address. */
+	uint32_t ip_src_mask_len;      /**< length of source address mask. */
+	uint32_t ip_dst;               /**< IPv4 dest address. */
+	uint32_t ip_dst_mask_len;      /**< length of dest address mask. */
+	uint16_t port_src_low;         /**< L4 source port low. */
+        uint16_t port_src_high;        /**< L4 source port high. */
+        uint16_t port_dst_low;         /**< L4 destination port low. */
+        uint16_t port_dst_high;        /**< L4 destination port high. */
+};
+
+struct ipv4_rule {
+	uint32_t id;
+	enum xdp_action action;
+	struct ipv4_5tuple_rule rule;
+	uint64_t num_packet;
+};
+
+/**
+ * IPv6 header fields we are searching through:
+ * - IP protocol value (8 bits)
+ * - IP source address (128 bits)
+ * - IP destination address (128 bits)
+ * - L4 protocol (UDP/TCP) source port number (16 bits)
+ * - L4 protocol (UDP/TCP) destination port number (16 bits)
+ */
+
+#define IPV6_ADDR_LEN   16
+#define IPV6_ADDR_U16   (IPV6_ADDR_LEN / sizeof(uint16_t))
+#define IPV6_ADDR_U32   (IPV6_ADDR_LEN / sizeof(uint32_t))
+#define IPV6_ADDR_U64   (IPV6_ADDR_LEN / sizeof(uint64_t))
+
+struct ipv6_5tuple {
+	uint8_t  proto;
+	uint32_t ip_src[IPV6_ADDR_U32];
+	uint32_t ip_dst[IPV6_ADDR_U32];
+	uint16_t port_src;
+	uint16_t port_dst;
+};
+
+#endif /* _ACL_XDP_H_ */
diff --git a/app/acl-bpf/main.c b/app/acl-bpf/main.c
new file mode 100644
index 0000000000..c2e613780a
--- /dev/null
+++ b/app/acl-bpf/main.c
@@ -0,0 +1,1311 @@
+/* SPDX-License-Identifier: BSD-3-Clause
+ * Copyright(c) 2010-2014 Intel Corporation
+ */
+
+#include <rte_string_fns.h>
+#include <rte_acl.h>
+#include <getopt.h>
+#include <string.h>
+#include <unistd.h>
+#include <bpf/bpf.h>
+
+#include <rte_cycles.h>
+#include <rte_per_lcore.h>
+#include <rte_lcore.h>
+#include <rte_ip.h>
+
+#include "acl_xdp.h"
+
+#define	PRINT_USAGE_START	"%s [EAL options] --\n"
+
+#define	RTE_LOGTYPE_TESTACL	RTE_LOGTYPE_USER1
+
+#define	APP_NAME	"TESTACL"
+
+#define GET_CB_FIELD(in, fd, base, lim, dlm)	do {            \
+	unsigned long val;                                      \
+	char *end_fld;                                          \
+	errno = 0;                                              \
+	val = strtoul((in), &end_fld, (base));                  \
+	if (errno != 0 || end_fld[0] != (dlm) || val > (lim))   \
+		return -EINVAL;                               \
+	(fd) = (typeof(fd))val;                                 \
+	(in) = end_fld + 1;                                     \
+} while (0)
+
+#define	OPT_RULE_FILE		"rulesf"
+#define	OPT_TRACE_FILE		"tracef"
+#define	OPT_RULE_NUM		"rulenum"
+#define	OPT_TRACE_NUM		"tracenum"
+#define	OPT_MAX_SIZE		"maxsize"
+#define	OPT_ITER_NUM		"iter"
+#define	OPT_VERBOSE		"verbose"
+#define	OPT_IPV6		"ipv6"
+#define	OPT_BPF			"bpf"
+
+#define	TRACE_DEFAULT_NUM	0x10000
+
+#define	RULE_NUM		0x10000
+
+#define COMMENT_LEAD_CHAR	'#'
+
+enum {
+	DUMP_NONE,
+	DUMP_SEARCH,
+	DUMP_PKT,
+	DUMP_MAX
+};
+
+enum {
+	IPV6_FRMT_NONE,
+	IPV6_FRMT_U32,
+	IPV6_FRMT_U64,
+};
+
+struct bpf_conf {	
+	struct rte_acl_bpf_id bpid;
+	struct rte_acl_bpf_fd bpfd;
+};
+
+static struct {
+	const char         *prgname;
+	const char         *rule_file;
+	const char         *trace_file;
+	size_t              max_size;
+	uint32_t            bld_categories;
+	uint32_t            run_categories;
+	uint32_t            nb_rules;
+	uint32_t            nb_traces;
+	uint32_t            trace_sz;
+	uint32_t            iter_num;
+	uint32_t            verbose;
+	uint32_t            ipv6;
+	uint32_t            used_traces;
+	void               *traces;
+	struct rte_acl_ctx *acx;
+	struct bpf_conf bpf;
+} config = {
+	.bld_categories = 3,
+	.run_categories = 1,
+	.nb_rules = RULE_NUM,
+	.nb_traces = TRACE_DEFAULT_NUM,
+	.iter_num = 1,
+	.verbose = DUMP_MAX,
+	.ipv6 = IPV6_FRMT_NONE,
+};
+
+static struct rte_acl_param prm = {
+	.name = APP_NAME,
+	.socket_id = SOCKET_ID_ANY,
+};
+
+/*
+ * Rule and trace formats definitions.
+ */
+
+enum {
+	PROTO_FIELD_IPV4,
+	SRC_FIELD_IPV4,
+	DST_FIELD_IPV4,
+	SRCP_FIELD_IPV4,
+	DSTP_FIELD_IPV4,
+	NUM_FIELDS_IPV4
+};
+
+/*
+ * That effectively defines order of IPV4VLAN classifications:
+ *  - PROTO
+ *  - VLAN (TAG and DOMAIN)
+ *  - SRC IP ADDRESS
+ *  - DST IP ADDRESS
+ *  - PORTS (SRC and DST)
+ */
+enum {
+	RTE_ACL_IPV4VLAN_PROTO,
+	RTE_ACL_IPV4VLAN_VLAN,
+	RTE_ACL_IPV4VLAN_SRC,
+	RTE_ACL_IPV4VLAN_DST,
+	RTE_ACL_IPV4VLAN_PORTS,
+	RTE_ACL_IPV4VLAN_NUM
+};
+
+struct rte_acl_field_def ipv4_defs[NUM_FIELDS_IPV4] = {
+	{
+		.type = RTE_ACL_FIELD_TYPE_BITMASK,
+		.size = sizeof(uint8_t),
+		.field_index = PROTO_FIELD_IPV4,
+		.input_index = RTE_ACL_IPV4VLAN_PROTO,
+		.offset = offsetof(struct ipv4_5tuple, proto),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_MASK,
+		.size = sizeof(uint32_t),
+		.field_index = SRC_FIELD_IPV4,
+		.input_index = RTE_ACL_IPV4VLAN_SRC,
+		.offset = offsetof(struct ipv4_5tuple, ip_src),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_MASK,
+		.size = sizeof(uint32_t),
+		.field_index = DST_FIELD_IPV4,
+		.input_index = RTE_ACL_IPV4VLAN_DST,
+		.offset = offsetof(struct ipv4_5tuple, ip_dst),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_RANGE,
+		.size = sizeof(uint16_t),
+		.field_index = SRCP_FIELD_IPV4,
+		.input_index = RTE_ACL_IPV4VLAN_PORTS,
+		.offset = offsetof(struct ipv4_5tuple, port_src),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_RANGE,
+		.size = sizeof(uint16_t),
+		.field_index = DSTP_FIELD_IPV4,
+		.input_index = RTE_ACL_IPV4VLAN_PORTS,
+		.offset = offsetof(struct ipv4_5tuple, port_dst),
+	},
+};
+
+/* treat IPV6 address as uint32_t[4] (default mode) */
+enum {
+	PROTO_FIELD_IPV6,
+	SRC1_FIELD_IPV6,
+	SRC2_FIELD_IPV6,
+	SRC3_FIELD_IPV6,
+	SRC4_FIELD_IPV6,
+	DST1_FIELD_IPV6,
+	DST2_FIELD_IPV6,
+	DST3_FIELD_IPV6,
+	DST4_FIELD_IPV6,
+	SRCP_FIELD_IPV6,
+	DSTP_FIELD_IPV6,
+	NUM_FIELDS_IPV6
+};
+
+/* treat IPV6 address as uint64_t[2] (default mode) */
+enum {
+	PROTO_FIELD_IPV6_U64,
+	SRC1_FIELD_IPV6_U64,
+	SRC2_FIELD_IPV6_U64,
+	DST1_FIELD_IPV6_U64,
+	DST2_FIELD_IPV6_U64,
+	SRCP_FIELD_IPV6_U64,
+	DSTP_FIELD_IPV6_U64,
+	NUM_FIELDS_IPV6_U64
+};
+
+enum {
+	PROTO_INDEX_IPV6_U64 = PROTO_FIELD_IPV6_U64,
+	SRC1_INDEX_IPV6_U64 = SRC1_FIELD_IPV6_U64,
+	SRC2_INDEX_IPV6_U64 = SRC2_FIELD_IPV6_U64 + 1,
+	DST1_INDEX_IPV6_U64 = DST1_FIELD_IPV6_U64 + 2,
+	DST2_INDEX_IPV6_U64 = DST2_FIELD_IPV6_U64 + 3,
+	PRT_INDEX_IPV6_U64 = SRCP_FIELD_IPV6 + 4,
+};
+
+struct rte_acl_field_def ipv6_defs[NUM_FIELDS_IPV6] = {
+	{
+		.type = RTE_ACL_FIELD_TYPE_BITMASK,
+		.size = sizeof(uint8_t),
+		.field_index = PROTO_FIELD_IPV6,
+		.input_index = PROTO_FIELD_IPV6,
+		.offset = offsetof(struct ipv6_5tuple, proto),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_MASK,
+		.size = sizeof(uint32_t),
+		.field_index = SRC1_FIELD_IPV6,
+		.input_index = SRC1_FIELD_IPV6,
+		.offset = offsetof(struct ipv6_5tuple, ip_src[0]),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_MASK,
+		.size = sizeof(uint32_t),
+		.field_index = SRC2_FIELD_IPV6,
+		.input_index = SRC2_FIELD_IPV6,
+		.offset = offsetof(struct ipv6_5tuple, ip_src[1]),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_MASK,
+		.size = sizeof(uint32_t),
+		.field_index = SRC3_FIELD_IPV6,
+		.input_index = SRC3_FIELD_IPV6,
+		.offset = offsetof(struct ipv6_5tuple, ip_src[2]),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_MASK,
+		.size = sizeof(uint32_t),
+		.field_index = SRC4_FIELD_IPV6,
+		.input_index = SRC4_FIELD_IPV6,
+		.offset = offsetof(struct ipv6_5tuple, ip_src[3]),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_MASK,
+		.size = sizeof(uint32_t),
+		.field_index = DST1_FIELD_IPV6,
+		.input_index = DST1_FIELD_IPV6,
+		.offset = offsetof(struct ipv6_5tuple, ip_dst[0]),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_MASK,
+		.size = sizeof(uint32_t),
+		.field_index = DST2_FIELD_IPV6,
+		.input_index = DST2_FIELD_IPV6,
+		.offset = offsetof(struct ipv6_5tuple, ip_dst[1]),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_MASK,
+		.size = sizeof(uint32_t),
+		.field_index = DST3_FIELD_IPV6,
+		.input_index = DST3_FIELD_IPV6,
+		.offset = offsetof(struct ipv6_5tuple, ip_dst[2]),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_MASK,
+		.size = sizeof(uint32_t),
+		.field_index = DST4_FIELD_IPV6,
+		.input_index = DST4_FIELD_IPV6,
+		.offset = offsetof(struct ipv6_5tuple, ip_dst[3]),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_RANGE,
+		.size = sizeof(uint16_t),
+		.field_index = SRCP_FIELD_IPV6,
+		.input_index = SRCP_FIELD_IPV6,
+		.offset = offsetof(struct ipv6_5tuple, port_src),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_RANGE,
+		.size = sizeof(uint16_t),
+		.field_index = DSTP_FIELD_IPV6,
+		.input_index = SRCP_FIELD_IPV6,
+		.offset = offsetof(struct ipv6_5tuple, port_dst),
+	},
+};
+
+struct rte_acl_field_def ipv6_u64_defs[NUM_FIELDS_IPV6_U64] = {
+	{
+		.type = RTE_ACL_FIELD_TYPE_BITMASK,
+		.size = sizeof(uint8_t),
+		.field_index = PROTO_FIELD_IPV6_U64,
+		.input_index = PROTO_FIELD_IPV6_U64,
+		.offset = offsetof(struct ipv6_5tuple, proto),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_MASK,
+		.size = sizeof(uint64_t),
+		.field_index = SRC1_FIELD_IPV6_U64,
+		.input_index = SRC1_INDEX_IPV6_U64,
+		.offset = offsetof(struct ipv6_5tuple, ip_src[0]),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_MASK,
+		.size = sizeof(uint64_t),
+		.field_index = SRC2_FIELD_IPV6_U64,
+		.input_index = SRC2_INDEX_IPV6_U64,
+		.offset = offsetof(struct ipv6_5tuple, ip_src[2]),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_MASK,
+		.size = sizeof(uint64_t),
+		.field_index = DST1_FIELD_IPV6_U64,
+		.input_index = DST1_INDEX_IPV6_U64,
+		.offset = offsetof(struct ipv6_5tuple, ip_dst[0]),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_MASK,
+		.size = sizeof(uint64_t),
+		.field_index = DST2_FIELD_IPV6_U64,
+		.input_index = DST2_INDEX_IPV6_U64,
+		.offset = offsetof(struct ipv6_5tuple, ip_dst[2]),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_RANGE,
+		.size = sizeof(uint16_t),
+		.field_index = SRCP_FIELD_IPV6_U64,
+		.input_index = PRT_INDEX_IPV6_U64,
+		.offset = offsetof(struct ipv6_5tuple, port_src),
+	},
+	{
+		.type = RTE_ACL_FIELD_TYPE_RANGE,
+		.size = sizeof(uint16_t),
+		.field_index = DSTP_FIELD_IPV6_U64,
+		.input_index = PRT_INDEX_IPV6_U64,
+		.offset = offsetof(struct ipv6_5tuple, port_dst),
+	},
+};
+
+enum {
+	CB_FLD_SRC_ADDR,
+	CB_FLD_DST_ADDR,
+	CB_FLD_SRC_PORT_LOW,
+	CB_FLD_SRC_PORT_DLM,
+	CB_FLD_SRC_PORT_HIGH,
+	CB_FLD_DST_PORT_LOW,
+	CB_FLD_DST_PORT_DLM,
+	CB_FLD_DST_PORT_HIGH,
+	CB_FLD_PROTO,
+	CB_FLD_NUM,
+};
+
+enum {
+	CB_TRC_SRC_ADDR,
+	CB_TRC_DST_ADDR,
+	CB_TRC_SRC_PORT,
+	CB_TRC_DST_PORT,
+	CB_TRC_PROTO,
+	CB_TRC_NUM,
+};
+
+RTE_ACL_RULE_DEF(acl_rule, RTE_ACL_MAX_FIELDS);
+
+static const char cb_port_delim[] = ":";
+
+static char line[LINE_MAX];
+
+#define	dump_verbose(lvl, fh, fmt, ...)	do { \
+	if ((lvl) <= (int32_t)config.verbose)        \
+		fprintf(fh, fmt, ##__VA_ARGS__);         \
+} while (0)
+
+
+/*
+ * Parse ClassBench input trace (test vectors and expected results) file.
+ * Expected format:
+ * <src_ipv4_addr> <space> <dst_ipv4_addr> <space> \
+ * <src_port> <space> <dst_port> <space> <proto>
+ */
+static int
+parse_cb_ipv4_trace(char *str, struct ipv4_5tuple *v)
+{
+	int i;
+	char *s, *sp, *in[CB_TRC_NUM];
+	static const char *dlm = " \t\n";
+
+	s = str;
+	for (i = 0; i != RTE_DIM(in); i++) {
+		in[i] = strtok_r(s, dlm, &sp);
+		if (in[i] == NULL)
+			return -EINVAL;
+		s = NULL;
+	}
+
+	GET_CB_FIELD(in[CB_TRC_SRC_ADDR], v->ip_src, 0, UINT32_MAX, 0);
+	GET_CB_FIELD(in[CB_TRC_DST_ADDR], v->ip_dst, 0, UINT32_MAX, 0);
+	GET_CB_FIELD(in[CB_TRC_SRC_PORT], v->port_src, 0, UINT16_MAX, 0);
+	GET_CB_FIELD(in[CB_TRC_DST_PORT], v->port_dst, 0, UINT16_MAX, 0);
+	GET_CB_FIELD(in[CB_TRC_PROTO], v->proto, 0, UINT8_MAX, 0);
+
+	/* convert to network byte order. */
+	v->ip_src = rte_cpu_to_be_32(v->ip_src);
+	v->ip_dst = rte_cpu_to_be_32(v->ip_dst);
+	v->port_src = rte_cpu_to_be_16(v->port_src);
+	v->port_dst = rte_cpu_to_be_16(v->port_dst);
+
+	return 0;
+}
+
+static int
+parse_cb_ipv6_addr_trace(const char *in, uint32_t v[IPV6_ADDR_U32])
+{
+	if (inet_pton(AF_INET6, in, v) != 1)
+		return -EINVAL;
+
+	return 0;
+}
+
+/*
+ * Parse ClassBench input trace (test vectors and expected results) file.
+ * Expected format:
+ * <src_ipv6_addr> <space> <dst_ipv6_addr> <space> \
+ * <src_port> <space> <dst_port> <space> <proto>
+ */
+static int
+parse_cb_ipv6_trace(char *str, struct ipv6_5tuple *v)
+{
+	int32_t i, rc;
+	char *s, *sp, *in[CB_TRC_NUM];
+	static const char *dlm = " \t\n";
+
+	s = str;
+	for (i = 0; i != RTE_DIM(in); i++) {
+		in[i] = strtok_r(s, dlm, &sp);
+		if (in[i] == NULL)
+			return -EINVAL;
+		s = NULL;
+	}
+
+	/* get ip6 src address. */
+	rc = parse_cb_ipv6_addr_trace(in[CB_TRC_SRC_ADDR], v->ip_src);
+	if (rc != 0)
+		return rc;
+
+	/* get ip6 dst address. */
+	rc = parse_cb_ipv6_addr_trace(in[CB_TRC_DST_ADDR], v->ip_dst);
+	if (rc != 0)
+		return rc;
+
+	GET_CB_FIELD(in[CB_TRC_SRC_PORT], v->port_src, 0, UINT16_MAX, 0);
+	GET_CB_FIELD(in[CB_TRC_DST_PORT], v->port_dst, 0, UINT16_MAX, 0);
+	GET_CB_FIELD(in[CB_TRC_PROTO], v->proto, 0, UINT8_MAX, 0);
+
+	/* convert to network byte order. */
+	v->port_src = rte_cpu_to_be_16(v->port_src);
+	v->port_dst = rte_cpu_to_be_16(v->port_dst);
+
+	return 0;
+}
+
+/* Bypass comment and empty lines */
+static int
+skip_line(const char *buf)
+{
+	uint32_t i;
+
+	for (i = 0; isspace(buf[i]) != 0; i++)
+		;
+
+	if (buf[i] == 0 || buf[i] == COMMENT_LEAD_CHAR)
+		return 1;
+
+	return 0;
+}
+
+static void
+tracef_init(void)
+{
+	static const char name[] = APP_NAME;
+	FILE *f;
+	size_t sz;
+	uint32_t i, k, n;
+	struct ipv4_5tuple *v;
+	struct ipv6_5tuple *w;
+
+	sz = config.nb_traces * (config.ipv6 ? sizeof(*w) : sizeof(*v));
+	config.traces = rte_zmalloc_socket(name, sz, RTE_CACHE_LINE_SIZE,
+			SOCKET_ID_ANY);
+	if (config.traces == NULL)
+		rte_exit(EXIT_FAILURE, "Cannot allocate %zu bytes for "
+			"requested %u number of trace records\n",
+			sz, config.nb_traces);
+
+	f = fopen(config.trace_file, "r");
+	if (f == NULL)
+		rte_exit(-EINVAL, "failed to open file: %s\n",
+			config.trace_file);
+
+	v = config.traces;
+	w = config.traces;
+	k = 0;
+	n = 0;
+	for (i = 0; n != config.nb_traces; i++) {
+
+		if (fgets(line, sizeof(line), f) == NULL)
+			break;
+
+		if (skip_line(line) != 0) {
+			k++;
+			continue;
+		}
+
+		n = i - k;
+
+		if (config.ipv6) {
+			if (parse_cb_ipv6_trace(line, w + n) != 0)
+				rte_exit(EXIT_FAILURE,
+					"%s: failed to parse ipv6 trace "
+					"record at line %u\n",
+					config.trace_file, i + 1);
+		} else {
+			if (parse_cb_ipv4_trace(line, v + n) != 0)
+				rte_exit(EXIT_FAILURE,
+					"%s: failed to parse ipv4 trace "
+					"record at line %u\n",
+					config.trace_file, i + 1);
+		}
+	}
+
+	config.used_traces = i - k;
+	fclose(f);
+}
+
+static int
+parse_ipv6_u32_net(char *in, struct rte_acl_field field[IPV6_ADDR_U32])
+{
+	char *sa, *sm, *sv;
+	uint32_t i, m, v[IPV6_ADDR_U32];
+
+	const char *dlm = "/";
+	const uint32_t nbu32 = sizeof(uint32_t) * CHAR_BIT;
+
+	/* get address. */
+	sv = NULL;
+	sa = strtok_r(in, dlm, &sv);
+	if (sa == NULL)
+		return -EINVAL;
+	sm = strtok_r(NULL, dlm, &sv);
+	if (sm == NULL)
+		return -EINVAL;
+
+	if (inet_pton(AF_INET6, sa, v) != 1)
+		return -EINVAL;
+
+	v[0] = rte_be_to_cpu_32(v[0]);
+	v[1] = rte_be_to_cpu_32(v[1]);
+	v[2] = rte_be_to_cpu_32(v[2]);
+	v[3] = rte_be_to_cpu_32(v[3]);
+
+	/* get mask. */
+	GET_CB_FIELD(sm, m, 0, CHAR_BIT * sizeof(v), 0);
+
+	/* put all together. */
+	for (i = 0; i != RTE_DIM(v); i++) {
+		if (m >= (i + 1) * nbu32)
+			field[i].mask_range.u32 = nbu32;
+		else
+			field[i].mask_range.u32 = m > (i * nbu32) ?
+				m - (i * nbu32) : 0;
+
+		field[i].value.u32 = v[i];
+	}
+
+	return 0;
+}
+
+static int
+parse_ipv6_u64_net(char *in, struct rte_acl_field field[IPV6_ADDR_U64])
+{
+	char *sa, *sm, *sv;
+	uint32_t i, m;
+	uint64_t v[IPV6_ADDR_U64];
+
+	const char *dlm = "/";
+	const uint32_t nbu64 = sizeof(uint64_t) * CHAR_BIT;
+
+	/* get address. */
+	sv = NULL;
+	sa = strtok_r(in, dlm, &sv);
+	if (sa == NULL)
+		return -EINVAL;
+	sm = strtok_r(NULL, dlm, &sv);
+	if (sm == NULL)
+		return -EINVAL;
+
+	if (inet_pton(AF_INET6, sa, v) != 1)
+		return -EINVAL;
+
+	v[0] = rte_be_to_cpu_64(v[0]);
+	v[1] = rte_be_to_cpu_64(v[1]);
+
+	/* get mask. */
+	GET_CB_FIELD(sm, m, 0, CHAR_BIT * sizeof(v), 0);
+
+	/* put all together. */
+	for (i = 0; i != RTE_DIM(v); i++) {
+		if (m >= (i + 1) * nbu64)
+			field[i].mask_range.u32 = nbu64;
+		else
+			field[i].mask_range.u32 = m > (i * nbu64) ?
+				m - (i * nbu64) : 0;
+
+		field[i].value.u64 = v[i];
+	}
+
+	return 0;
+}
+
+static int
+parse_cb_ipv6_rule(char *str, struct acl_rule *v, int frmt)
+{
+	int i, rc;
+	uint32_t fidx;
+	const uint32_t *field_map;
+	char *s, *sp, *in[CB_FLD_NUM];
+	int (*parse_ipv6_net)(char *s, struct rte_acl_field f[]);
+
+	static const char *dlm = " \t\n";
+
+	static const uint32_t field_map_u32[CB_FLD_NUM] = {
+		[CB_FLD_SRC_ADDR] = SRC1_FIELD_IPV6,
+		[CB_FLD_DST_ADDR] = DST1_FIELD_IPV6,
+		[CB_FLD_SRC_PORT_LOW] = SRCP_FIELD_IPV6,
+		[CB_FLD_SRC_PORT_HIGH] = SRCP_FIELD_IPV6,
+		[CB_FLD_DST_PORT_LOW] = DSTP_FIELD_IPV6,
+		[CB_FLD_DST_PORT_HIGH] = DSTP_FIELD_IPV6,
+		[CB_FLD_PROTO] = PROTO_FIELD_IPV6,
+	};
+
+	static const uint32_t field_map_u64[CB_FLD_NUM] = {
+		[CB_FLD_SRC_ADDR] = SRC1_FIELD_IPV6_U64,
+		[CB_FLD_DST_ADDR] = DST1_FIELD_IPV6_U64,
+		[CB_FLD_SRC_PORT_LOW] = SRCP_FIELD_IPV6_U64,
+		[CB_FLD_SRC_PORT_HIGH] = SRCP_FIELD_IPV6_U64,
+		[CB_FLD_DST_PORT_LOW] = DSTP_FIELD_IPV6_U64,
+		[CB_FLD_DST_PORT_HIGH] = DSTP_FIELD_IPV6_U64,
+		[CB_FLD_PROTO] = PROTO_FIELD_IPV6_U64,
+	};
+
+	if (frmt == IPV6_FRMT_U32) {
+		field_map = field_map_u32;
+		parse_ipv6_net = parse_ipv6_u32_net;
+	} else if (frmt == IPV6_FRMT_U64) {
+		field_map = field_map_u64;
+		parse_ipv6_net = parse_ipv6_u64_net;
+	} else
+		return -ENOTSUP;
+
+	/*
+	 * Skip leading '@'
+	 */
+	if (strchr(str, '@') != str)
+		return -EINVAL;
+
+	s = str + 1;
+
+	for (i = 0; i != RTE_DIM(in); i++) {
+		in[i] = strtok_r(s, dlm, &sp);
+		if (in[i] == NULL)
+			return -EINVAL;
+		s = NULL;
+	}
+
+	fidx = CB_FLD_SRC_ADDR;
+	rc = parse_ipv6_net(in[fidx], v->field + field_map[fidx]);
+	if (rc != 0) {
+		RTE_LOG(ERR, TESTACL,
+			"failed to read source address/mask: %s\n", in[fidx]);
+		return rc;
+	}
+
+	fidx = CB_FLD_DST_ADDR;
+	rc = parse_ipv6_net(in[fidx], v->field + field_map[fidx]);
+	if (rc != 0) {
+		RTE_LOG(ERR, TESTACL,
+			"failed to read destination address/mask: %s\n",
+			in[fidx]);
+		return rc;
+	}
+
+	/* source port. */
+	fidx = CB_FLD_SRC_PORT_LOW;
+	GET_CB_FIELD(in[fidx], v->field[field_map[fidx]].value.u16,
+		0, UINT16_MAX, 0);
+
+	fidx = CB_FLD_SRC_PORT_HIGH;
+	GET_CB_FIELD(in[fidx], v->field[field_map[fidx]].mask_range.u16,
+		0, UINT16_MAX, 0);
+
+	if (strncmp(in[CB_FLD_SRC_PORT_DLM], cb_port_delim,
+			sizeof(cb_port_delim)) != 0)
+		return -EINVAL;
+
+	/* destination port. */
+	fidx = CB_FLD_DST_PORT_LOW;
+	GET_CB_FIELD(in[fidx], v->field[field_map[fidx]].value.u16,
+		0, UINT16_MAX, 0);
+
+	fidx = CB_FLD_DST_PORT_HIGH;
+	GET_CB_FIELD(in[fidx], v->field[field_map[fidx]].mask_range.u16,
+		0, UINT16_MAX, 0);
+
+	if (strncmp(in[CB_FLD_DST_PORT_DLM], cb_port_delim,
+			sizeof(cb_port_delim)) != 0)
+		return -EINVAL;
+
+	fidx = CB_FLD_PROTO;
+	GET_CB_FIELD(in[fidx], v->field[field_map[fidx]].value.u8,
+		0, UINT8_MAX, '/');
+	GET_CB_FIELD(in[fidx], v->field[field_map[fidx]].mask_range.u8,
+		0, UINT8_MAX, 0);
+
+	return 0;
+}
+
+static int
+parse_cb_ipv6_u32_rule(char *str, struct acl_rule *v)
+{
+	return parse_cb_ipv6_rule(str, v, IPV6_FRMT_U32);
+}
+
+static int
+parse_cb_ipv6_u64_rule(char *str, struct acl_rule *v)
+{
+	return parse_cb_ipv6_rule(str, v, IPV6_FRMT_U64);
+}
+
+static int
+parse_ipv4_net(char *in, uint32_t *addr, uint32_t *mask_len)
+{
+	char *sa, *sm, *sv;
+	uint32_t m, v;
+
+	const char *dlm = "/";
+
+	sv = NULL;
+	sa = strtok_r(in, dlm, &sv);
+	if (sa == NULL)
+		return -EINVAL;
+	sm = strtok_r(NULL, dlm, &sv);
+	if (sm == NULL)
+		return -EINVAL;
+
+	if (inet_pton(AF_INET, sa, &v) != 1)
+		return -EINVAL;
+
+	addr[0] = rte_be_to_cpu_32(v);
+
+	GET_CB_FIELD(sm, m, 0, sizeof(uint32_t) * CHAR_BIT, 0);
+	mask_len[0] = m;
+
+	return 0;
+}
+/*
+ * Parse ClassBench rules file.
+ * Expected format:
+ * '@'<src_ipv4_addr>'/'<masklen> <space> \
+ * <dst_ipv4_addr>'/'<masklen> <space> \
+ * <src_port_low> <space> ":" <src_port_high> <space> \
+ * <dst_port_low> <space> ":" <dst_port_high> <space> \
+ * <proto>'/'<mask>
+ */
+static int
+parse_cb_ipv4_rule(char *str, struct acl_rule *v)
+{
+	int i, rc;
+	char *s, *sp, *in[CB_FLD_NUM];
+	static const char *dlm = " \t\n";
+
+	/*
+	 * Skip leading '@'
+	 */
+	if (strchr(str, '@') != str)
+		return -EINVAL;
+
+	s = str + 1;
+
+	for (i = 0; i != RTE_DIM(in); i++) {
+		in[i] = strtok_r(s, dlm, &sp);
+		if (in[i] == NULL)
+			return -EINVAL;
+		s = NULL;
+	}
+
+	rc = parse_ipv4_net(in[CB_FLD_SRC_ADDR],
+			&v->field[SRC_FIELD_IPV4].value.u32,
+			&v->field[SRC_FIELD_IPV4].mask_range.u32);
+	if (rc != 0) {
+		RTE_LOG(ERR, TESTACL,
+			"failed to read source address/mask: %s\n",
+			in[CB_FLD_SRC_ADDR]);
+		return rc;
+	}
+
+	rc = parse_ipv4_net(in[CB_FLD_DST_ADDR],
+			&v->field[DST_FIELD_IPV4].value.u32,
+			&v->field[DST_FIELD_IPV4].mask_range.u32);
+	if (rc != 0) {
+		RTE_LOG(ERR, TESTACL,
+			"failed to read destination address/mask: %s\n",
+			in[CB_FLD_DST_ADDR]);
+		return rc;
+	}
+
+	/* source port. */
+	GET_CB_FIELD(in[CB_FLD_SRC_PORT_LOW],
+		v->field[SRCP_FIELD_IPV4].value.u16,
+		0, UINT16_MAX, 0);
+	GET_CB_FIELD(in[CB_FLD_SRC_PORT_HIGH],
+		v->field[SRCP_FIELD_IPV4].mask_range.u16,
+		0, UINT16_MAX, 0);
+
+	if (strncmp(in[CB_FLD_SRC_PORT_DLM], cb_port_delim,
+			sizeof(cb_port_delim)) != 0)
+		return -EINVAL;
+
+	/* destination port. */
+	GET_CB_FIELD(in[CB_FLD_DST_PORT_LOW],
+		v->field[DSTP_FIELD_IPV4].value.u16,
+		0, UINT16_MAX, 0);
+	GET_CB_FIELD(in[CB_FLD_DST_PORT_HIGH],
+		v->field[DSTP_FIELD_IPV4].mask_range.u16,
+		0, UINT16_MAX, 0);
+
+	if (strncmp(in[CB_FLD_DST_PORT_DLM], cb_port_delim,
+			sizeof(cb_port_delim)) != 0)
+		return -EINVAL;
+
+	GET_CB_FIELD(in[CB_FLD_PROTO], v->field[PROTO_FIELD_IPV4].value.u8,
+		0, UINT8_MAX, '/');
+	GET_CB_FIELD(in[CB_FLD_PROTO], v->field[PROTO_FIELD_IPV4].mask_range.u8,
+		0, UINT8_MAX, 0);
+
+	return 0;
+}
+
+typedef int (*parse_5tuple)(char *text, struct acl_rule *rule);
+
+static int
+acl_add_map_ipv4_rule(int32_t mapfd, const struct acl_rule *p,
+	enum xdp_action action)
+{
+	int32_t rc;
+	struct ipv4_rule rule;
+
+	/* convert rule to xdp map format */
+	memset(&rule, 0, sizeof(rule));
+	rule.id = p->data.userdata;
+	rule.action = action;
+
+	/* proto */
+	rule.rule.proto = p->field[PROTO_FIELD_IPV4].value.u8;
+	rule.rule.proto_mask = p->field[PROTO_FIELD_IPV4].mask_range.u8;
+
+	/* src ip and mask */
+	rule.rule.ip_src = p->field[SRC_FIELD_IPV4].value.u32;
+	rule.rule.ip_src_mask_len = p->field[SRC_FIELD_IPV4].mask_range.u32;
+
+	/* dst ip and mask */
+	rule.rule.ip_dst = p->field[DST_FIELD_IPV4].value.u32;
+	rule.rule.ip_dst_mask_len = p->field[DST_FIELD_IPV4].mask_range.u32;
+
+	/* src L4 ports */
+	rule.rule.port_src_low = p->field[SRCP_FIELD_IPV4].value.u16;
+	rule.rule.port_src_high = p->field[SRCP_FIELD_IPV4].mask_range.u16;
+
+	/* dst L4 ports */
+	rule.rule.port_dst_low = p->field[DSTP_FIELD_IPV4].value.u16;
+	rule.rule.port_dst_high = p->field[DSTP_FIELD_IPV4].mask_range.u16;
+
+	rc = bpf_map_update_elem(mapfd, &rule.id, &rule, BPF_ANY);
+	if (rc != 0) {
+		printf("%s:%d  "
+			"bpf_map_update_elem(fd=%d, key=%u "
+			" failed, rc=%d, errno=%d\n",
+			__func__, __LINE__,
+			mapfd, rule.id, rc, errno);
+	}
+
+	return rc;
+}
+
+static int
+add_cb_rules(FILE *f, struct rte_acl_ctx *ctx)
+{
+	int rc;
+	uint32_t i, k, n;
+	struct acl_rule v;
+	parse_5tuple parser;
+
+	static const parse_5tuple parser_func[] = {
+		[IPV6_FRMT_NONE] = parse_cb_ipv4_rule,
+		[IPV6_FRMT_U32] = parse_cb_ipv6_u32_rule,
+		[IPV6_FRMT_U64] = parse_cb_ipv6_u64_rule,
+	};
+
+	memset(&v, 0, sizeof(v));
+	parser = parser_func[config.ipv6];
+
+	/* result zero means no match was found, so initialize 'no match' rule
+	 * with default action: PASS
+	 */
+	rc = acl_add_map_ipv4_rule(config.bpf.bpfd.rule_fd, &v, XDP_PASS);
+	if (rc != 0) {
+		RTE_LOG(ERR, TESTACL, "failed to add default rule "
+			"into XDP table, error code: %d (%s)\n",
+			rc, strerror(-rc));
+		return rc;
+	}
+
+	k = 0;
+	for (i = 1; fgets(line, sizeof(line), f) != NULL; i++) {
+
+		if (skip_line(line) != 0) {
+			k++;
+			continue;
+		}
+
+		n = i - k;
+		rc = parser(line, &v);
+		if (rc != 0) {
+			RTE_LOG(ERR, TESTACL, "line %u: parse_cb_ipv4vlan_rule"
+				" failed, error code: %d (%s)\n",
+				i, rc, strerror(-rc));
+			return rc;
+		}
+
+		v.data.category_mask = RTE_LEN2MASK(RTE_ACL_MAX_CATEGORIES,
+			typeof(v.data.category_mask));
+		v.data.priority = RTE_ACL_MAX_PRIORITY - n;
+		v.data.userdata = n;
+
+		rc = rte_acl_add_rules(ctx, (struct rte_acl_rule *)&v, 1);
+		if (rc != 0) {
+			RTE_LOG(ERR, TESTACL, "line %u: failed to add rules "
+				"into ACL context, error code: %d (%s)\n",
+				i, rc, strerror(-rc));
+			return rc;
+		}
+		rc = acl_add_map_ipv4_rule(config.bpf.bpfd.rule_fd, &v,
+			XDP_DROP);
+		if (rc != 0) {
+			RTE_LOG(ERR, TESTACL, "line %u: failed to add rules "
+				"into XDP table, error code: %d (%s)\n",
+				i, rc, strerror(-rc));
+			return rc;
+		}
+	}
+
+	return 0;
+}
+
+static void
+print_bpf_conf(const struct bpf_conf *cfg)
+{
+	printf("bpf_onf={\n");
+	printf("\tctx_id=%u,\n", cfg->bpid.ctx_id);
+	printf("\ttrans_id=%u,\n", cfg->bpid.trans_id);
+	printf("\trule_id=%u,\n", cfg->bpid.rule_id);
+	printf("\tctx_fd=%d,\n", cfg->bpfd.ctx_fd);
+	printf("\ttrans_fd=%d,\n", cfg->bpfd.trans_fd);
+	printf("\trule_fd=%d,\n", cfg->bpfd.rule_fd);
+	printf("}\n");
+}
+
+static void
+acx_init(void)
+{
+	int ret;
+	FILE *f;
+	struct rte_acl_config cfg;
+
+	memset(&cfg, 0, sizeof(cfg));
+
+	/* setup ACL build config. */
+	if (config.ipv6 == IPV6_FRMT_U32) {
+		cfg.num_fields = RTE_DIM(ipv6_defs);
+		memcpy(&cfg.defs, ipv6_defs, sizeof(ipv6_defs));
+	} else if (config.ipv6 == IPV6_FRMT_U64) {
+		cfg.num_fields = RTE_DIM(ipv6_u64_defs);
+		memcpy(&cfg.defs, ipv6_u64_defs, sizeof(ipv6_u64_defs));
+	} else {
+		cfg.num_fields = RTE_DIM(ipv4_defs);
+		memcpy(&cfg.defs, ipv4_defs, sizeof(ipv4_defs));
+	}
+	cfg.num_categories = config.bld_categories;
+	cfg.max_size = config.max_size;
+
+	/* setup ACL creation parameters. */
+	prm.rule_size = RTE_ACL_RULE_SZ(cfg.num_fields);
+	prm.max_rule_num = config.nb_rules;
+
+	config.acx = rte_acl_create(&prm);
+	if (config.acx == NULL)
+		rte_exit(rte_errno, "failed to create ACL context\n");
+
+	/* add ACL rules. */
+	f = fopen(config.rule_file, "r");
+	if (f == NULL)
+		rte_exit(-EINVAL, "failed to open file %s\n",
+			config.rule_file);
+
+	ret = add_cb_rules(f, config.acx);
+	if (ret != 0)
+		rte_exit(ret, "failed to add rules into ACL context\n");
+
+	fclose(f);
+
+	/* perform build. */
+	ret = rte_acl_build(config.acx, &cfg);
+
+	dump_verbose(DUMP_NONE, stdout,
+		"rte_acl_build(%u) finished with %d\n",
+		config.bld_categories, ret);
+
+	rte_acl_dump(config.acx);
+
+	ret = rte_acl_bpf_fill(&config.bpf.bpfd, config.acx);
+
+	if (ret != 0)
+		rte_exit(ret, "failed to build search context\n");
+}
+
+static uint32_t
+search_ip5tuples_once(uint32_t categories, uint32_t step)
+{
+	int ret;
+	uint32_t i, j, k, n, r;
+	const uint8_t *data[step], *v;
+	uint32_t results[step * categories];
+
+	v = config.traces;
+	for (i = 0; i != config.used_traces; i += n) {
+
+		n = RTE_MIN(step, config.used_traces - i);
+
+		for (j = 0; j != n; j++) {
+			data[j] = v;
+			v += config.trace_sz;
+		}
+
+		ret = 0;
+		for (j = 0; j != n && ret == 0; j++)
+			ret = rte_acl_bpf_classify(&config.bpf.bpfd,
+				data[j], config.trace_sz, results + j);
+
+		if (ret != 0)
+			rte_exit(ret, "classify for ipv%c_5tuples returns %d\n",
+				config.ipv6 ? '6' : '4', ret);
+
+		for (r = 0, j = 0; j != n; j++) {
+			for (k = 0; k != categories; k++, r++) {
+				dump_verbose(DUMP_PKT, stdout,
+					"ipv%c_5tuple: %u, category: %u, "
+					"result: %u\n",
+					config.ipv6 ? '6' : '4',
+					i + j + 1, k, results[r] - 1);
+			}
+
+		}
+	}
+
+	dump_verbose(DUMP_SEARCH, stdout,
+		"%s(%u, %u) returns %u\n", __func__,
+		categories, step, i);
+	return i;
+}
+
+static int
+search_ip5tuples(__rte_unused void *arg)
+{
+	uint64_t pkt, start, tm;
+	uint32_t i, lcore;
+	long double st;
+
+	lcore = rte_lcore_id();
+	start = rte_rdtsc_precise();
+	pkt = 0;
+
+	for (i = 0; i != config.iter_num; i++) {
+		pkt += search_ip5tuples_once(config.run_categories, 1);
+	}
+
+	tm = rte_rdtsc_precise() - start;
+
+	st = (long double)tm / rte_get_timer_hz();
+	dump_verbose(DUMP_NONE, stdout,
+		"%s  @lcore %u: %" PRIu32 " iterations, %" PRIu64 " pkts, %"
+		PRIu32 " categories, %" PRIu64 " cycles (%.2Lf sec), "
+		"%.2Lf cycles/pkt, %.2Lf pkt/sec\n",
+		__func__, lcore, i, pkt,
+		config.run_categories, tm, st,
+		(pkt == 0) ? 0 : (long double)tm / pkt, pkt / st);
+
+	return 0;
+}
+
+static unsigned long
+get_ulong_opt(const char *opt, const char *name, size_t min, size_t max)
+{
+	unsigned long val;
+	char *end;
+
+	errno = 0;
+	val = strtoul(opt, &end, 0);
+	if (errno != 0 || end[0] != 0 || val > max || val < min)
+		rte_exit(-EINVAL, "invalid value: \"%s\" for option: %s\n",
+			opt, name);
+	return val;
+}
+
+static int
+get_bpf_ids(struct bpf_conf *bpf, const char *opt)
+{
+	GET_CB_FIELD(opt, bpf->bpid.ctx_id, 0, UINT32_MAX, ':');
+	GET_CB_FIELD(opt, bpf->bpid.trans_id, 0, UINT32_MAX, ':');
+	GET_CB_FIELD(opt, bpf->bpid.rule_id, 0, UINT32_MAX, 0);
+	return 0;
+}
+
+static void
+get_bpf_opt(struct bpf_conf *bpf, const char *opt, const char *name)
+{
+	int32_t rc;
+
+	memset(bpf, 0, sizeof(*bpf));
+
+	rc = get_bpf_ids(bpf, opt);
+	if (rc != 0)
+		rte_exit(rc, "invalid value: \"%s\" for option: %s\n",
+			opt, name);
+}
+
+static void
+get_ipv6_opt(const char *opt, const char *name)
+{
+	uint32_t i;
+
+	static const struct {
+		const char *name;
+		uint32_t val;
+	} ipv6_opt[] = {
+		{
+			.name = "4B",
+			.val = IPV6_FRMT_U32,
+		},
+		{
+			.name = "8B",
+			.val = IPV6_FRMT_U64,
+		},
+	};
+
+	for (i = 0; i != RTE_DIM(ipv6_opt); i++) {
+		if (strcmp(opt, ipv6_opt[i].name) == 0) {
+			config.ipv6 = ipv6_opt[i].val;
+			return;
+		}
+	}
+
+	rte_exit(-EINVAL, "invalid value: \"%s\" for option: %s\n",
+		opt, name);
+}
+
+
+static void
+print_usage(const char *prgname)
+{
+	fprintf(stdout,
+		PRINT_USAGE_START
+		"--" OPT_RULE_FILE "=<rules set file>\n"
+		"[--" OPT_TRACE_FILE "=<input traces file>]\n"
+		"[--" OPT_RULE_NUM
+			"=<maximum number of rules for ACL context>]\n"
+		"[--" OPT_TRACE_NUM
+			"=<number of traces to read binary file in>]\n"
+		"[--" OPT_MAX_SIZE
+			"=<size limit (in bytes) for runtime ACL structures> "
+			"leave 0 for default behaviour]\n"
+		"[--" OPT_ITER_NUM "=<number of iterations to perform>]\n"
+		"[--" OPT_VERBOSE "=<verbose level>]\n"
+		"[--" OPT_IPV6 "(=4B | 8B) <IPv6 rules and trace files>]\n"
+		"[--" OPT_BPF
+			"(=test | <ctx_map_id:trans_map_id>:<rules_map_id>)\n",
+		prgname);
+}
+
+static void
+dump_config(FILE *f)
+{
+	fprintf(f, "%s:\n", __func__);
+	fprintf(f, "%s:%s\n", OPT_RULE_FILE, config.rule_file);
+	fprintf(f, "%s:%s\n", OPT_TRACE_FILE, config.trace_file);
+	fprintf(f, "%s:%u\n", OPT_RULE_NUM, config.nb_rules);
+	fprintf(f, "%s:%u\n", OPT_TRACE_NUM, config.nb_traces);
+	fprintf(f, "%s:%zu\n", OPT_MAX_SIZE, config.max_size);
+	fprintf(f, "%s:%u\n", OPT_ITER_NUM, config.iter_num);
+	fprintf(f, "%s:%u\n", OPT_VERBOSE, config.verbose);
+	fprintf(f, "%s:%u\n", OPT_IPV6, config.ipv6);
+}
+
+static void
+check_config(void)
+{
+	if (config.rule_file == NULL) {
+		print_usage(config.prgname);
+		rte_exit(-EINVAL, "mandatory option %s is not specified\n",
+			OPT_RULE_FILE);
+	}
+}
+
+
+static void
+get_input_opts(int argc, char **argv)
+{
+	static struct option lgopts[] = {
+		{OPT_RULE_FILE, 1, 0, 0},
+		{OPT_TRACE_FILE, 1, 0, 0},
+		{OPT_TRACE_NUM, 1, 0, 0},
+		{OPT_RULE_NUM, 1, 0, 0},
+		{OPT_MAX_SIZE, 1, 0, 0},
+		{OPT_ITER_NUM, 1, 0, 0},
+		{OPT_VERBOSE, 1, 0, 0},
+		{OPT_IPV6, 2, 0, 0},
+		{OPT_BPF, 1, 0, 0},
+		{NULL, 0, 0, 0}
+	};
+
+	int opt, opt_idx;
+
+	while ((opt = getopt_long(argc, argv, "", lgopts,  &opt_idx)) != EOF) {
+
+		if (opt != 0) {
+			print_usage(config.prgname);
+			rte_exit(-EINVAL, "unknown option: %c", opt);
+		}
+
+		if (strcmp(lgopts[opt_idx].name, OPT_RULE_FILE) == 0) {
+			config.rule_file = optarg;
+		} else if (strcmp(lgopts[opt_idx].name, OPT_TRACE_FILE) == 0) {
+			config.trace_file = optarg;
+		} else if (strcmp(lgopts[opt_idx].name, OPT_RULE_NUM) == 0) {
+			config.nb_rules = get_ulong_opt(optarg,
+				lgopts[opt_idx].name, 1, RTE_ACL_MAX_INDEX + 1);
+		} else if (strcmp(lgopts[opt_idx].name, OPT_MAX_SIZE) == 0) {
+			config.max_size = get_ulong_opt(optarg,
+				lgopts[opt_idx].name, 0, SIZE_MAX);
+		} else if (strcmp(lgopts[opt_idx].name, OPT_TRACE_NUM) == 0) {
+			config.nb_traces = get_ulong_opt(optarg,
+				lgopts[opt_idx].name, 1, UINT32_MAX);
+		} else if (strcmp(lgopts[opt_idx].name, OPT_ITER_NUM) == 0) {
+			config.iter_num = get_ulong_opt(optarg,
+				lgopts[opt_idx].name, 1, INT32_MAX);
+		} else if (strcmp(lgopts[opt_idx].name, OPT_VERBOSE) == 0) {
+			config.verbose = get_ulong_opt(optarg,
+				lgopts[opt_idx].name, DUMP_NONE, DUMP_MAX);
+		} else if (strcmp(lgopts[opt_idx].name, OPT_IPV6) == 0) {
+			config.ipv6 = IPV6_FRMT_U32;
+			if (optarg != NULL)
+				get_ipv6_opt(optarg, lgopts[opt_idx].name);
+		} else if (strcmp(lgopts[opt_idx].name, OPT_BPF) == 0) {
+			get_bpf_opt(&config.bpf, optarg, lgopts[opt_idx].name);
+		}
+	}
+	config.trace_sz = config.ipv6 ? sizeof(struct ipv6_5tuple) :
+						sizeof(struct ipv4_5tuple);
+
+}
+
+int
+main(int argc, char **argv)
+{
+	int ret;
+
+	ret = rte_eal_init(argc, argv);
+	if (ret < 0)
+		rte_panic("Cannot init EAL\n");
+
+	argc -= ret;
+	argv += ret;
+
+	config.prgname = argv[0];
+
+	get_input_opts(argc, argv);
+	dump_config(stdout);
+	check_config();
+
+	ret = rte_acl_bpf_open(&config.bpf.bpid, &config.bpf.bpfd);
+	print_bpf_conf(&config.bpf);
+	if (ret != 0)
+		return ret;
+
+	acx_init();
+
+	if (config.trace_file != NULL)
+		tracef_init();
+
+	search_ip5tuples(NULL);
+
+	rte_acl_bpf_close(&config.bpf.bpfd);
+	rte_acl_free(config.acx);
+	return 0;
+}
diff --git a/app/acl-bpf/meson.build b/app/acl-bpf/meson.build
new file mode 100644
index 0000000000..c0dc4b221a
--- /dev/null
+++ b/app/acl-bpf/meson.build
@@ -0,0 +1,11 @@
+# SPDX-License-Identifier: BSD-3-Clause
+# Copyright(c) 2019 Intel Corporation
+
+if is_windows
+    build = false
+    reason = 'not supported on Windows'
+    subdir_done()
+endif
+
+sources = files('main.c')
+deps += ['acl', 'net']
diff --git a/app/acl-bpf/xdp_acl4.c b/app/acl-bpf/xdp_acl4.c
new file mode 100644
index 0000000000..09a3c32b1b
--- /dev/null
+++ b/app/acl-bpf/xdp_acl4.c
@@ -0,0 +1,275 @@
+#include <linux/bpf.h>
+#include <bpf/bpf_helpers.h>
+#include <linux/if_ether.h>
+#include <arpa/inet.h>
+#include <linux/ip.h>
+#include <linux/ipv6.h>
+#include <linux/udp.h>
+#include <linux/tcp.h>
+
+#include <limits.h>
+#include <stdint.h>
+#include "acl_bpf.h"
+#include "acl_internal.h"
+#include "acl_xdp.h"
+
+/*
+ * to perform packet filtering XDP-ACL needs 3 MAPs fille by user-space:
+ * acl_ctx - contains some data about generated ACL conext
+ * acl_trans - contains actual 'transitions' array, i.e. generated and
+ * compressed trie representation.
+ * acl_rule - contains actual rules to match.
+ */
+struct {
+        __uint(type, BPF_MAP_TYPE_ARRAY);
+        __type(key, uint32_t);
+        __type(value, struct rte_acl_bpf_ctx);
+        __uint(max_entries, 1);
+} acl_ctx SEC(".maps");
+
+struct {
+        __uint(type, BPF_MAP_TYPE_ARRAY);
+        __type(key, uint32_t);
+        __type(value, uint64_t);
+        __uint(max_entries, 0x400000);
+} acl_trans SEC(".maps");
+
+struct {
+        __uint(type, BPF_MAP_TYPE_ARRAY);
+        __type(key, uint32_t);
+        __type(value, struct ipv4_rule);
+        __uint(max_entries, 0x10000);
+} acl_rule SEC(".maps");
+
+
+union packet_data {
+	struct ipv4_5tuple data;
+	uint8_t raw[sizeof(struct ipv4_5tuple)];
+};
+
+#define DIM(a)	(sizeof(a) / sizeof(a[0]))
+
+/*
+ * Here we perform search through transitions array, exactly in the same
+ * way it is done by lib/acl/acl_run_scalar.c, except that we always
+ * search one packet at a time.
+ * As a rule of thumb:
+ * - if match is found, it returns XDP_DROP
+ * - else if we realize that no match for given packet exists,
+ *   it returns XDP_PASS
+ * - else return XDP_ABORT, which means that the search shall continue to
+ *   the next level of the trie.
+ */
+static enum xdp_action
+resolve_match(uint32_t match_index, uint64_t trans)
+{
+	uint32_t idx;
+	union rte_acl_bpf_match *match;
+	struct ipv4_rule *rule;
+
+	trans &= RTE_ACL_NODE_INDEX;
+	idx = match_index + trans;
+
+	/* get match record */
+	match = bpf_map_lookup_elem(&acl_trans, &idx);
+	if (match == NULL) {
+		return XDP_PASS;
+	}
+
+	/* find corresponding rule record */
+	idx = match->result;
+	//bpf_printk("%s:%d *idx=%u\n", __func__, __LINE__, idx);
+	rule = bpf_map_lookup_elem(&acl_rule, &idx);
+	if (rule == NULL)
+		return XDP_PASS;
+
+	/* update rule stats */
+	__atomic_fetch_add(&rule->num_packet, 1, __ATOMIC_RELAXED);
+
+	return rule->action;
+}
+
+static inline uint32_t
+scan_forward(uint32_t input, uint32_t max)
+{
+	return (input == 0) ? max : __builtin_ctz(input);
+}
+
+static uint32_t
+resolve_next_index(uint64_t transition, uint8_t input)
+{
+	uint32_t addr, index, ranges, x, a, b, c;
+
+	/* break transition into component parts */
+	ranges = transition >> (sizeof(index) * CHAR_BIT);
+	index = transition & ~RTE_ACL_NODE_INDEX;
+	addr = transition ^ index;
+
+	if (index != RTE_ACL_NODE_DFA) {
+		/* calc address for a QRANGE/SINGLE node */
+		c = (uint32_t)input * SCALAR_QRANGE_MULT;
+		a = ranges | SCALAR_QRANGE_MIN;
+		a -= (c & SCALAR_QRANGE_MASK);
+		b = c & SCALAR_QRANGE_MIN;
+		a &= SCALAR_QRANGE_MIN;
+		a ^= (ranges ^ b) & (a ^ b);
+		x = scan_forward(a, 32) >> 3;
+	} else {
+		/* calc address for a DFA node */
+		x = ranges >> (input /
+			RTE_ACL_DFA_GR64_SIZE * RTE_ACL_DFA_GR64_BIT);
+		x &= UINT8_MAX;
+		x = input - x;
+	}
+
+	addr += x;
+	return addr;
+}
+
+static inline int
+one_step_trans(uint32_t match_index, uint8_t input, uint64_t trans,
+	uint64_t *next) 
+{
+	uint32_t idx;
+	enum xdp_action action;
+	const uint64_t *val;
+
+	idx = resolve_next_index(trans, input);
+	val = bpf_map_lookup_elem(&acl_trans, &idx);
+	if (val == NULL)
+		return XDP_PASS;
+	trans = *val;
+	/* if match is found */
+	if ((trans & RTE_ACL_NODE_MATCH) != 0) {
+		action = resolve_match(match_index, trans);
+		//bpf_printk("%s:%d *action=%d\n", __func__, __LINE__, action);
+		return action;
+	}
+	*next = trans;
+	return XDP_ABORTED;
+}
+
+
+SEC("xdp_prog")
+int xdp_acl_prog1(struct xdp_md *ctx)
+{
+	void *data_end;
+	void *data;
+	int32_t rc;
+	enum xdp_action action;
+	uint32_t i, idx, input, iphlen, match_index, ofs;
+	uint64_t trans;
+	const uint64_t *val;
+	struct ethhdr *eth;
+	struct iphdr *iph;
+	struct udphdr *udph;
+	union packet_data pd;
+	const struct rte_acl_bpf_ctx *bcx;
+
+	data_end = (void *)(long)ctx->data_end;
+	data = (void *)(long)ctx->data;
+
+	eth = data;
+	if (data + sizeof(*eth) > data_end)
+        	return XDP_DROP;
+
+	if (eth->h_proto != htons(ETH_P_IP))
+		return XDP_PASS;
+
+	iph = (struct iphdr *)(eth + 1);
+	if ((void *)(iph + 1) > data_end)
+		return XDP_DROP;
+
+	pd.data.proto = iph->protocol;
+	pd.data.ip_src = iph->saddr;
+	pd.data.ip_dst = iph->daddr;
+
+	iphlen = iph->ihl * sizeof(uint32_t);
+	udph = (struct udphdr *)((uint8_t *)iph + iphlen);
+
+	if((void *)(udph + 1) > data_end)
+		return XDP_PASS;
+
+	pd.data.port_src = udph->source;
+	pd.data.port_dst = udph->dest;
+
+	/* start search with IP proto */
+	i = 0;
+	bcx = bpf_map_lookup_elem(&acl_ctx, &i);
+	if (bcx == NULL)
+		return XDP_PASS;
+
+	match_index = bcx->match_index;
+
+	input = pd.data.proto;
+	idx = bcx->trie[0].root_index + input;
+	val = bpf_map_lookup_elem(&acl_trans, &idx);
+	if (val == NULL)
+		return XDP_PASS;
+	trans = *val;
+	/* if match is found */
+	if ((trans & RTE_ACL_NODE_MATCH) != 0) {
+		action = resolve_match(match_index, trans);
+		return action;
+	}
+
+	/* continue search with IP src addr */
+	ofs = offsetof(struct ipv4_5tuple, ip_src);
+	rc = one_step_trans(match_index, pd.raw[ofs], trans, &trans); 
+	if (rc != XDP_ABORTED)
+		return rc;
+
+	rc = one_step_trans(match_index, pd.raw[ofs + 1], trans, &trans); 
+	if (rc != XDP_ABORTED)
+		return rc;
+
+	rc = one_step_trans(match_index, pd.raw[ofs + 2], trans, &trans); 
+	if (rc != XDP_ABORTED)
+		return rc;
+
+	rc = one_step_trans(match_index, pd.raw[ofs + 3], trans, &trans); 
+	if (rc != XDP_ABORTED)
+		return rc;
+
+	/* continue search with IP dest addr */
+	ofs = offsetof(struct ipv4_5tuple, ip_dst);
+	rc = one_step_trans(match_index, pd.raw[ofs], trans, &trans); 
+	if (rc != XDP_ABORTED)
+		return rc;
+
+	rc = one_step_trans(match_index, pd.raw[ofs + 1], trans, &trans); 
+	if (rc != XDP_ABORTED)
+		return rc;
+
+	rc = one_step_trans(match_index, pd.raw[ofs + 2], trans, &trans); 
+	if (rc != XDP_ABORTED)
+		return rc;
+
+	rc = one_step_trans(match_index, pd.raw[ofs + 3], trans, &trans); 
+	if (rc != XDP_ABORTED)
+		return rc;
+
+	/* continue search with L4 src port number */
+	ofs = offsetof(struct ipv4_5tuple, port_src);
+	rc = one_step_trans(match_index, pd.raw[ofs], trans, &trans); 
+	if (rc != XDP_ABORTED)
+		return rc;
+
+	rc = one_step_trans(match_index, pd.raw[ofs + 1], trans, &trans); 
+	if (rc != XDP_ABORTED)
+		return rc;
+
+	/* continue search with L4 dest port number */
+	ofs = offsetof(struct ipv4_5tuple, port_dst);
+	rc = one_step_trans(match_index, pd.raw[ofs], trans, &trans); 
+	if (rc != XDP_ABORTED)
+		return rc;
+
+	rc = one_step_trans(match_index, pd.raw[ofs + 1], trans, &trans); 
+	if (rc != XDP_ABORTED)
+		return rc;
+	
+	return XDP_PASS;
+}
+
+char _license[] SEC("license") = "GPL";
diff --git a/app/meson.build b/app/meson.build
index e2db888ae1..0af3b4d606 100644
--- a/app/meson.build
+++ b/app/meson.build
@@ -17,6 +17,7 @@ if enable_apps.length() == 0
 endif
 
 apps = [
+        'acl-bpf',
         'dumpcap',
         'graph',
         'pdump',
diff --git a/lib/acl/acl_bpf.c b/lib/acl/acl_bpf.c
new file mode 100644
index 0000000000..176827e455
--- /dev/null
+++ b/lib/acl/acl_bpf.c
@@ -0,0 +1,345 @@
+/* SPDX-License-Identifier: BSD-3-Clause
+ * Copyright(c) 2010-2014 Intel Corporation
+ */
+
+#include <rte_acl.h>
+#include "acl.h"
+#include "acl_log.h"
+#include "acl_run.h"
+#include "acl_bpf.h"
+
+#include <bpf/bpf.h>
+#include <unistd.h>
+
+static void
+acl_bpf_reset(struct rte_acl_bpf_fd *btx)
+{
+	btx->trans_fd = -1;
+	btx->ctx_fd = -1;
+	btx->rule_fd = -1;
+}
+
+static void
+fill_bpf_ctx(struct rte_acl_bpf_ctx *btx, const struct rte_acl_ctx *ctx)
+{
+	uint32_t i, j, k, n;
+
+	memset(btx, 0, sizeof(*btx));
+
+	n = ctx->match_index + ctx->num_rules + 1;
+	btx->num_trans = n;
+	btx->match_index = ctx->match_index;
+	btx->num_matches = ctx->num_rules + 1;
+
+	/* copy tries */
+	btx->num_tries = ctx->num_tries;
+	for (i = 0; i != btx->num_tries; i++) {
+		btx->trie[i].root_index = ctx->trie[i].root_index;
+		n = 4 * (ctx->trie[i].num_data_indexes - 1) + 1;
+		btx->trie[i].num_offset = n;
+		btx->trie[i].data_offset[0] = ctx->trie[i].data_index[0];
+		for (j = 1; j != ctx->trie[i].num_data_indexes; j++) {
+			k = 4 * (j - 1) + 1;
+			n = ctx->trie[i].data_index[j];
+			btx->trie[i].data_offset[k] = n;
+			btx->trie[i].data_offset[k + 1] = n + 1;
+			btx->trie[i].data_offset[k + 2] = n + 2;
+			btx->trie[i].data_offset[k + 3] = n + 3;
+		}
+	}
+}
+
+static int
+fill_ctx_map(const struct rte_acl_bpf_fd *bpx,  const struct rte_acl_ctx *ctx)
+{
+	int32_t rc;
+	uint32_t i;
+	struct rte_acl_bpf_ctx btx;
+
+	fill_bpf_ctx(&btx, ctx);
+
+	i = 0;
+	rc = bpf_map_update_elem(bpx->ctx_fd, &i, &btx, BPF_ANY);
+	if (rc != 0)
+		printf("%s:%d  "
+			"bpf_map_update_elem(fd=%d, key=%u, val=%p"
+			" failed, rc=%d, errno=%d\n",
+			__func__, __LINE__,
+			bpx->ctx_fd, i, &btx, rc, errno);
+	return rc;
+}
+
+static int
+fill_trans_map(const struct rte_acl_bpf_fd *bpx,  const struct rte_acl_ctx *ctx)
+{
+	int32_t rc;
+	uint32_t i, j, k;
+	struct rte_acl_bpf_ctx btx;
+	union rte_acl_bpf_match bpf_match;
+	const struct rte_acl_match_results *match;
+
+	fill_bpf_ctx(&btx, ctx);
+
+	rc = 0;
+	for (i = 0; i != btx.match_index && rc == 0; i++) {
+		rc = bpf_map_update_elem(bpx->trans_fd, &i,
+				ctx->trans_table + i, BPF_ANY);
+	}
+
+	match = ((const struct rte_acl_match_results *)(ctx->trans_table + i));
+	for(j = 0; j != btx.num_matches && rc == 0; j++) {
+		k = i + j;
+		bpf_match.result = match[j].results[0];
+		bpf_match.priority = match[j].priority[0];
+		rc = bpf_map_update_elem(bpx->trans_fd, &k, &bpf_match.raw,
+			BPF_ANY);
+	}
+
+	if (rc != 0)
+		printf("%s:%d  "
+			"bpf_map_update_elem(fd=%d, key=%u, val=%" PRIx64
+			" failed, rc=%d, errno=%d\n",
+			__func__, __LINE__,
+			bpx->trans_fd, i, ctx->trans_table[i],
+			rc, errno);
+
+	return rc;
+}
+
+int
+rte_acl_bpf_open(const struct rte_acl_bpf_id *bpid, struct rte_acl_bpf_fd *bpfd)
+{
+	int32_t rc;
+
+	/* set all file descriptors to -1 */
+	acl_bpf_reset(bpfd);
+
+	bpfd->ctx_fd = bpf_map_get_fd_by_id(bpid->ctx_id);
+	rc = (bpfd->ctx_fd < 0) ? -errno : 0;
+	printf("%s:%d  "
+		"bpf_map_get_fd(=\"%u\") returns %d, errno=%d\n",
+		__func__, __LINE__, bpid->ctx_id,  bpfd->ctx_fd, rc);
+
+	if (rc == 0) {
+		bpfd->trans_fd = bpf_map_get_fd_by_id(bpid->trans_id);
+		rc = (bpfd->trans_fd < 0) ? -errno : 0;
+		printf("%s:%d  "
+			"bpf_map_get_fd(=\"%u\") returns %d, errno=%d\n",
+			__func__, __LINE__,
+			bpid->trans_id,  bpfd->trans_fd, rc);
+	}
+
+	if (rc == 0) {
+		bpfd->rule_fd = bpf_map_get_fd_by_id(bpid->rule_id);
+		rc = (bpfd->rule_fd < 0) ? -errno : 0;
+		printf("%s:%d  "
+			"bpf_map_get_fd(=\"%u\") returns %d, errno=%d\n",
+			__func__, __LINE__,
+			bpid->rule_id,  bpfd->rule_fd, rc);
+	}
+
+	/* success */
+	if (rc == 0)
+		return 0;
+
+	rte_acl_bpf_close(bpfd);
+	return rc;
+}
+
+int
+rte_acl_bpf_fill(const struct rte_acl_bpf_fd *bpfd,
+	const struct rte_acl_ctx *ctx)
+{
+	int32_t rc;
+
+	if (ctx->num_tries > 1)
+		printf("%s:%d: !!! WARNING given ACL CTX uses %u tries, "
+			"curret ACL XDP program supports only CTX with "
+			"single trie - BPF matches might be invalid\n",
+			__func__, __LINE__, ctx->num_tries);
+
+
+	rc = fill_ctx_map(bpfd, ctx);
+
+	if (rc == 0)
+		rc = fill_trans_map(bpfd, ctx);
+
+	return rc;
+}
+
+void
+rte_acl_bpf_close(struct rte_acl_bpf_fd *bpx)
+{
+	close(bpx->trans_fd);
+	close(bpx->ctx_fd);
+	close(bpx->rule_fd);
+	acl_bpf_reset(bpx);
+}
+
+static int
+resolve_match(int32_t fd, uint32_t match_index, uint64_t trans,
+	 union rte_acl_bpf_match *match)
+{
+	int32_t rc;
+	uint32_t idx;
+
+	trans &= RTE_ACL_NODE_INDEX;
+	idx = match_index + trans;
+
+	/* get match record */
+	rc = bpf_map_lookup_elem(fd, &idx, match);
+	if (rc != 0) {
+		printf("%s:%d  "
+			"bpf_map_lokup_elem(fd=%d, key=%u) failed, "
+			" rc=%d, errno=%d\n",
+			__func__, __LINE__, fd, idx, rc, errno);
+		return rc;
+	}
+
+	return 0;
+}
+
+static inline uint32_t
+scan_forward(uint32_t input, uint32_t max)
+{
+	return (input == 0) ? max : rte_bsf32(input);
+}
+
+static uint32_t
+resolve_next_index(uint64_t transition, uint8_t input)
+{
+	uint32_t addr, index, ranges, x, a, b, c;
+
+	/* break transition into component parts */
+	ranges = transition >> (sizeof(index) * CHAR_BIT);
+	index = transition & ~RTE_ACL_NODE_INDEX;
+	addr = transition ^ index;
+
+	if (index != RTE_ACL_NODE_DFA) {
+		/* calc address for a QRANGE/SINGLE node */
+		c = (uint32_t)input * SCALAR_QRANGE_MULT;
+		a = ranges | SCALAR_QRANGE_MIN;
+		a -= (c & SCALAR_QRANGE_MASK);
+		b = c & SCALAR_QRANGE_MIN;
+		a &= SCALAR_QRANGE_MIN;
+		a ^= (ranges ^ b) & (a ^ b);
+		x = scan_forward(a, 32) >> 3;
+	} else {
+		/* calc address for a DFA node */
+		x = ranges >> (input /
+			RTE_ACL_DFA_GR64_SIZE * RTE_ACL_DFA_GR64_BIT);
+		x &= UINT8_MAX;
+		x = input - x;
+	}
+
+	addr += x;
+	return addr;
+}
+
+static int
+acl_trie_search(const struct rte_acl_bpf_ctx *btx, uint32_t trie_idx,
+	int32_t trans_fd, const uint8_t *data, uint32_t data_len,
+	union rte_acl_bpf_match *match)
+{
+	int32_t rc;
+	uint32_t data_ofs, i, idx;
+	uint64_t trans;
+	uint8_t input;
+
+	if (btx->trie[trie_idx].data_offset[0] >= data_len)
+		return -ERANGE;
+
+	i = 0;
+	data_ofs = btx->trie[trie_idx].data_offset[i];
+	input = data[data_ofs];
+	idx = btx->trie[trie_idx].root_index + input;
+
+	do {
+
+		/* get next transision */
+		rc = bpf_map_lookup_elem(trans_fd, &idx, &trans);
+		if (rc != 0)
+			break;
+
+		/* if match is found */
+		if ((trans & RTE_ACL_NODE_MATCH) != 0) {
+			rc = resolve_match(trans_fd, btx->match_index,
+				trans, match);
+			break;
+		}
+
+		/* read next data offset */
+		if (++i == btx->trie[trie_idx].num_offset)
+			break;
+
+		/* get next input byte */
+		data_ofs = btx->trie[trie_idx].data_offset[i];
+		if (data_ofs >= data_len) {
+			rc = -ERANGE;
+			break;
+		/* get next trans based on input */
+		} else {
+			input = data[data_ofs];
+			idx = resolve_next_index(trans, input);
+		}
+	} while (true);
+
+	if (rc != 0) {
+		printf("%s:%d  ERROR at transition lookup: "
+			"data_index=%u, data_offset=%u, data_value=%#hhx: "
+			"trans_fd=%d, key=%u failed, "
+			" rc=%d, errno=%d\n",
+			__func__, __LINE__,
+			i, data_ofs, input,
+			trans_fd, idx,
+			rc, errno);
+		return rc;
+	} else if (i == btx->trie[trie_idx].num_offset) {
+		printf("%s:%d(trans_fd=%d, data=%p, offset_idx=%u: "
+			"!!! NO MATCH WAS FOUND !!!",
+			__func__, __LINE__,  trans_fd, data, i);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int
+rte_acl_bpf_classify(const struct rte_acl_bpf_fd *bpx, const uint8_t *data,
+	uint32_t data_len, uint32_t *res)
+{
+	int32_t rc;
+	uint32_t i;
+	struct rte_acl_bpf_ctx btx;
+	union rte_acl_bpf_match cur, match;
+
+	if (bpx == NULL || data == NULL || data_len == 0 || res == 0)
+		return -EINVAL;
+
+	/* read CTX record */
+	i = 0;
+	rc = bpf_map_lookup_elem(bpx->ctx_fd, &i, &btx);
+	if (rc != 0) {
+		printf("%s:%d  "
+			"bpf_map_lokup_elem(fd=%d, key=%u) failed, "
+			" rc=%d, errno=%d\n",
+			__func__, __LINE__, bpx->ctx_fd, i, rc, errno);
+		return rc;
+	}
+
+	cur.priority = INT32_MIN;
+	cur.result = 0;
+	for (i = 0; i != btx.num_tries; i++) {
+		rc = acl_trie_search(&btx, i, bpx->trans_fd, data, data_len,
+			&match);
+		if (rc != 0)
+			return rc;	
+		/* if we found a match with greater priority, then use it */
+		if (match.priority >= cur.priority)
+			cur = match;
+	}
+
+	*res = cur.result;
+	return rc;
+}
+
diff --git a/lib/acl/acl_bpf.h b/lib/acl/acl_bpf.h
new file mode 100644
index 0000000000..75fe2d6722
--- /dev/null
+++ b/lib/acl/acl_bpf.h
@@ -0,0 +1,36 @@
+/* SPDX-License-Identifier: BSD-3-Clause
+ * Copyright(c) 2010-2014 Intel Corporation
+ */
+
+#ifndef _ACL_BPF_H_
+#define _ACL_BPF_H_
+
+/** MAX number of tries per one ACL BPF context.*/
+#define RTE_ACL_BPF_MAX_TRIES	8
+
+/** MAX bytes to classify for BPF */
+#define RTE_ACL_BPF_MAX_DATA_LEN	40
+
+struct rte_acl_bpf_trie {
+	uint32_t root_index;
+	uint32_t num_offset;
+	uint32_t data_offset[RTE_ACL_BPF_MAX_DATA_LEN];
+};
+
+struct rte_acl_bpf_ctx {
+	uint32_t num_trans;
+	uint32_t match_index;
+	uint32_t num_matches;
+	uint32_t num_tries;
+	struct rte_acl_bpf_trie trie[RTE_ACL_BPF_MAX_TRIES];
+};
+
+union rte_acl_bpf_match {
+	uint64_t raw;
+	struct {
+		uint32_t result;
+		int32_t priority;
+	};
+};
+
+#endif /* _ACL_BPF_H_ */
diff --git a/lib/acl/meson.build b/lib/acl/meson.build
index 9cba08321a..54057085d6 100644
--- a/lib/acl/meson.build
+++ b/lib/acl/meson.build
@@ -74,3 +74,18 @@ elif dpdk_conf.has('RTE_ARCH_ARM')
 elif dpdk_conf.has('RTE_ARCH_PPC_64')
     sources += files('acl_run_altivec.c')
 endif
+
+bpf_dep = dependency('libbpf', required: false, method: 'pkg-config')
+if not bpf_dep.found()
+    bpf_dep = cc.find_library('bpf', required: false)
+endif
+
+if bpf_dep.found()
+    dpdk_conf.set('RTE_LIBRTE_ACL_BPF', 1)
+    sources += files('acl_bpf.c')
+    ext_deps += bpf_dep
+else
+    warning('libbpf is missing, rte_acl_ BPF API will be disabled')
+endif
+
+
diff --git a/lib/acl/rte_acl.h b/lib/acl/rte_acl.h
index ca75a6f220..4226e135b5 100644
--- a/lib/acl/rte_acl.h
+++ b/lib/acl/rte_acl.h
@@ -355,6 +355,50 @@ rte_acl_dump(const struct rte_acl_ctx *ctx);
 void
 rte_acl_list_dump(void);
 
+/**
+ * BPF related API
+ */
+
+struct rte_acl_bpf_fd {
+	/** file desc for transition map */
+	int32_t trans_fd;
+	/** file desc for context map */
+	int32_t ctx_fd;
+	/** file desc for rules map */
+	int32_t rule_fd;
+};
+
+struct rte_acl_bpf_id {
+	/** ACL transitions BPF map ID */
+	uint32_t trans_id;
+	/** ACL context BPF map id */
+	uint32_t ctx_id;
+	/** rules BPF map ID */
+	int32_t rule_id;
+};
+
+/*
+ * Take as input ACL BPF MAP IDs and open corresponding file descriptor for
+ * each of them.
+ */
+int
+rte_acl_bpf_open(const struct rte_acl_bpf_id *bpid,
+	struct rte_acl_bpf_fd *bpfd);
+
+/*
+ * Fill BPF CTX and TRANS MAPs with contents of ACL context.
+ */
+int
+rte_acl_bpf_fill(const struct rte_acl_bpf_fd *bpfd,
+	const struct rte_acl_ctx *ctx);
+
+void
+rte_acl_bpf_close(struct rte_acl_bpf_fd *bpfd);
+
+int
+rte_acl_bpf_classify(const struct rte_acl_bpf_fd *bpfd, const uint8_t *data,
+        uint32_t data_len, uint32_t *res);
+
 #ifdef __cplusplus
 }
 #endif
-- 
2.43.0

